{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a158438b8e76d61ecd176acc01b0ec5842c85d73"
   },
   "source": [
    " # Introduction\n",
    "This notebook describes an approach to classify movies as either comedies or dramas based on plot synopsis.  As such, it attempts to solve a typical text classification problem, albeit perhaps a tricky one, since defining humor can be difficult even for humans..  Classifications and synopses are taken from Wikipedia;  supplemental training data is taken from the news category dataset, which contains headlines and short descriptions of Huffington Post articles, in addition to categories that include comedy.  I apply a simple Bag-of-Words approach, a CNN-based approach, and a hybrid CNN-LSTM approach to the problem.  I also use a few standard text-processing steps to pre-processing the data.  The simplest approach turns out to be the best; none of the more complicated approaches I tried outperformed the Bag-of-Words logistic regression model.  \n",
    "\n",
    "### Outline\n",
    "\n",
    "1. Import and define tools and functions.\n",
    "2.  Exploratory data analysis\n",
    "3.  Text processing and vectorization.\n",
    "4.  Bag-Of-Words Model\n",
    "5.  GloVE Embeddings\n",
    "6.  CNN Model\n",
    "7.  CNN-LSTM Model\n",
    "8.  Analysis and Conclusion\n",
    "\n",
    "> \"**Humor is kind of like pornography â€¦ you know it when you see it.**\" -Kevin Litman Navarro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a4f35543bbc0ad0bc3b671ecb06ef9737edab02"
   },
   "source": [
    "# 1. Import Tools****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "bf2b264fc603b226ef7b19170cc611961dc9fe34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import matplotlib\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout\n",
    "from keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout, Bidirectional\n",
    "from keras.layers import LSTM, Dropout,Activation, Bidirectional\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem.porter import *\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "e3bf9051cf7aca82f45fa4a13d3939e56e3164c3"
   },
   "outputs": [],
   "source": [
    "def removeStopWords(lowerArg):\n",
    "    i=0\n",
    "    removed=[]\n",
    "    for x in lowerArg:\n",
    "        i+=1\n",
    "        removed.append((' '.join([word for word in x.split() if word not in nltk.corpus.stopwords.words('english')])))\n",
    "        \n",
    "    return pd.Series(removed).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('../input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv')\n",
    "newsDF=pd.read_json('../input/news-category-dataset/News_Category_Dataset_v2.json', lines=True,dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a8a4579a710f22d2e18e16355cd0a0609f68534"
   },
   "source": [
    "# 2. EDA for Movie Data\n",
    "Quick initial look at the dataset.  Additional analysis (like counting unique words) will occur during processing and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e130036c084940023138ac4f05409055f68dc2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of words in synopses:  334\n",
      "Standard deviation number of words in synopses:  285\n",
      "Number of Dramas:  5964\n",
      "Number of Comedies:  4379\n"
     ]
    }
   ],
   "source": [
    "##Limit to just comedies and Genres\n",
    "genres=['drama','comedy']\n",
    "df=df[df['Genre'].isin(genres)]\n",
    "df=df.reset_index()\n",
    "df['GenreID']=df['Genre'].apply(lambda x: genres.index(x))\n",
    "\n",
    "wordCount=df['Plot'].apply(lambda x: x.count(' '))\n",
    "print(\"Mean number of words in synopses: \",int(wordCount.mean()))\n",
    "print(\"Standard deviation number of words in synopses: \", int(wordCount.std()))\n",
    "print('Number of Dramas: ',df['Genre'].value_counts()[0])\n",
    "print('Number of Comedies: ',df['Genre'].value_counts()[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "13b785f9c78bb9dd64719e912118ccb8883e72ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Synopsis Word Counts\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEnpJREFUeJzt3X+s3XV9x/HnaxTQqLFF7hrS1rXMZgaTic0NYDTGSVZ+LStLnGFZxg1r0mXDRZMtW53J8GeCS6aTZGNjo1s1TmSooZls2FWM2R8CRRGBir0ihDaFVgtszuiGvvfH+Vw81nu557a390c/z0dycj7f9/dzvt/Ph+9pX/1+z/ccUlVIkvrzc4s9AEnS4jAAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqdGCoAkK5PcluQbSfYleX2Ss5LsTrK/Pa9qfZPkhiSTSR5IsmloOxOt//4kEydrUpKk2Y16BvBR4N+r6tXAa4F9wHZgT1VtBPa0ZYDLgI3tsQ24ESDJWcB1wIXABcB1U6EhSVp4me2bwEleDtwPnFtDnZM8Ary5qg4lOQf4YlX9UpK/a+1PDvebelTV77X6T/Wbztlnn13r168/gelJUn/uu+++71TV2Gz9VoywrQ3AEeAfk7wWuA94B7C6qg61Pk8Cq1t7DfDE0OsPtNpM9RmtX7+evXv3jjBESdKUJI+P0m+US0ArgE3AjVX1OuB/+MnlHgDamcG8/KhQkm1J9ibZe+TIkfnYpCRpGqMEwAHgQFXd3ZZvYxAIT7VLP7Tnw239QWDd0OvXttpM9Z9SVTdV1XhVjY+NzXoGI0k6TrMGQFU9CTyR5Jda6WLgYWAXMHUnzwRwe2vvAq5udwNdBDzbLhXdCWxOsqp9+Lu51SRJi2CUzwAA/hD4RJIzgEeBaxiEx61JtgKPA29rfe8ALgcmge+3vlTV0STvB+5t/d5XVUfnZRaSpDmb9S6gxTQ+Pl5+CCxJc5Pkvqoan62f3wSWpE4ZAJLUKQNAkjplAEhSp0a9C2hZWr/9c4uy38euv2JR9itJc+EZgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6tRIAZDksSRfT3J/kr2tdlaS3Un2t+dVrZ4kNySZTPJAkk1D25lo/fcnmTg5U5IkjWIuZwC/UlXnV9V4W94O7KmqjcCetgxwGbCxPbYBN8IgMIDrgAuBC4DrpkJDkrTwTuQS0BZgZ2vvBK4cqn+sBr4MrExyDnAJsLuqjlbV08Bu4NIT2L8k6QSMGgAFfD7JfUm2tdrqqjrU2k8Cq1t7DfDE0GsPtNpMdUnSIlgxYr83VtXBJD8P7E7yjeGVVVVJaj4G1AJmG8ArX/nK+dikJGkaI50BVNXB9nwY+CyDa/hPtUs7tOfDrftBYN3Qy9e22kz1Y/d1U1WNV9X42NjY3GYjSRrZrAGQ5CVJXjbVBjYDDwK7gKk7eSaA21t7F3B1uxvoIuDZdqnoTmBzklXtw9/NrSZJWgSjXAJaDXw2yVT/f66qf09yL3Brkq3A48DbWv87gMuBSeD7wDUAVXU0yfuBe1u/91XV0XmbiSRpTmYNgKp6FHjtNPXvAhdPUy/g2hm2tQPYMfdhSpLmm98ElqROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6NXIAJDktyVeT/Gtb3pDk7iSTST6V5IxWP7MtT7b164e28a5WfyTJJfM9GUnS6OZyBvAOYN/Q8oeAj1TVq4Cnga2tvhV4utU/0vqR5DzgKuA1wKXA3yQ57cSGL0k6XiMFQJK1wBXAP7TlAG8BbmtddgJXtvaWtkxbf3HrvwW4pap+WFXfBiaBC+ZjEpKkuRv1DOCvgD8BftyWXwE8U1XPteUDwJrWXgM8AdDWP9v6P1+f5jWSpAU2awAk+TXgcFXdtwDjIcm2JHuT7D1y5MhC7FKSujTKGcAbgF9P8hhwC4NLPx8FViZZ0fqsBQ629kFgHUBb/3Lgu8P1aV7zvKq6qarGq2p8bGxszhOSJI1m1gCoqndV1dqqWs/gQ9wvVNVvA3cBb23dJoDbW3tXW6at/0JVVatf1e4S2gBsBO6Zt5lIkuZkxexdZvSnwC1JPgB8Fbi51W8GPp5kEjjKIDSoqoeS3Ao8DDwHXFtVPzqB/UuSTsCcAqCqvgh8sbUfZZq7eKrqB8BvzvD6DwIfnOsgJUnzz28CS1KnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdmjUAkrwoyT1JvpbkoSTvbfUNSe5OMpnkU0nOaPUz2/JkW79+aFvvavVHklxysiYlSZrdKGcAPwTeUlWvBc4HLk1yEfAh4CNV9SrgaWBr678VeLrVP9L6keQ84CrgNcClwN8kOW0+JyNJGt2sAVAD32uLp7dHAW8Bbmv1ncCVrb2lLdPWX5wkrX5LVf2wqr4NTAIXzMssJElzNtJnAElOS3I/cBjYDXwLeKaqnmtdDgBrWnsN8ARAW/8s8Irh+jSvkSQtsJECoKp+VFXnA2sZ/Kv91SdrQEm2JdmbZO+RI0dO1m4kqXtzuguoqp4B7gJeD6xMsqKtWgscbO2DwDqAtv7lwHeH69O8ZngfN1XVeFWNj42NzWV4kqQ5GOUuoLEkK1v7xcCvAvsYBMFbW7cJ4PbW3tWWaeu/UFXV6le1u4Q2ABuBe+ZrIpKkuVkxexfOAXa2O3Z+Dri1qv41ycPALUk+AHwVuLn1vxn4eJJJ4CiDO3+oqoeS3Ao8DDwHXFtVP5rf6UiSRjVrAFTVA8Drpqk/yjR38VTVD4DfnGFbHwQ+OPdhSpLmm98ElqROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkTo3yUxCao/XbP7co+33s+isWZb+SlifPACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTswZAknVJ7krycJKHkryj1c9KsjvJ/va8qtWT5IYkk0keSLJpaFsTrf/+JBMnb1qSpNmMcgbwHPBHVXUecBFwbZLzgO3AnqraCOxpywCXARvbYxtwIwwCA7gOuBC4ALhuKjQkSQtv1gCoqkNV9ZXW/m9gH7AG2ALsbN12Ale29hbgYzXwZWBlknOAS4DdVXW0qp4GdgOXzutsJEkjm9NnAEnWA68D7gZWV9WhtupJYHVrrwGeGHrZgVabqS5JWgQjB0CSlwKfBt5ZVf81vK6qCqj5GFCSbUn2Jtl75MiR+dikJGkaIwVAktMZ/OX/iar6TCs/1S7t0J4Pt/pBYN3Qy9e22kz1n1JVN1XVeFWNj42NzWUukqQ5GOUuoAA3A/uq6sNDq3YBU3fyTAC3D9WvbncDXQQ82y4V3QlsTrKqffi7udUkSYtgxQh93gD8DvD1JPe32p8B1wO3JtkKPA68ra27A7gcmAS+D1wDUFVHk7wfuLf1e19VHZ2XWUiS5mzWAKiq/wQyw+qLp+lfwLUzbGsHsGMuA5QknRx+E1iSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSp0b5LSAtE+u3f27R9v3Y9Vcs2r4lHR/PACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnZo1AJLsSHI4yYNDtbOS7E6yvz2vavUkuSHJZJIHkmwaes1E678/ycTJmY4kaVSjnAH8E3DpMbXtwJ6q2gjsacsAlwEb22MbcCMMAgO4DrgQuAC4bio0JEmLY9YAqKovAUePKW8Bdrb2TuDKofrHauDLwMok5wCXALur6mhVPQ3s5mdDRZK0gI73M4DVVXWotZ8EVrf2GuCJoX4HWm2muiRpkZzwh8BVVUDNw1gASLItyd4ke48cOTJfm5UkHeN4A+CpdmmH9ny41Q8C64b6rW21meo/o6puqqrxqhofGxs7zuFJkmZzvAGwC5i6k2cCuH2ofnW7G+gi4Nl2qehOYHOSVe3D382tJklaJCtm65Dkk8CbgbOTHGBwN8/1wK1JtgKPA29r3e8ALgcmge8D1wBU1dEk7wfubf3eV1XHfrAsSVpAswZAVf3WDKsunqZvAdfOsJ0dwI45jU6SdNL4TWBJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHVq1v8pvDSK9ds/tyj7fez6KxZlv9KpwDMASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1KkFD4AklyZ5JMlkku0LvX9J0sCCfg8gyWnAXwO/ChwA7k2yq6oeXshx6NSxWN8/AL+DoOVvoc8ALgAmq+rRqvpf4BZgywKPQZLEwn8TeA3wxNDyAeDCBR6DNC/89rOWuyX3UxBJtgHb2uL3kjxynJs6G/jO/IxqSXA+S9uCzScfOum78NgsbaPM5xdG2dBCB8BBYN3Q8tpWe15V3QTcdKI7SrK3qsZPdDtLhfNZ2k6l+ZxKcwHn80IW+jOAe4GNSTYkOQO4Cti1wGOQJLHAZwBV9VyStwN3AqcBO6rqoYUcgyRpYME/A6iqO4A7FmBXJ3wZaYlxPkvbqTSfU2ku4HxmlKqar21JkpYRfwpCkjp1SgbAcv25iSSPJfl6kvuT7G21s5LsTrK/Pa9q9SS5oc3xgSSbFnnsO5IcTvLgUG3OY08y0frvTzKxGHNp45huPu9JcrAdn/uTXD607l1tPo8kuWSovujvxSTrktyV5OEkDyV5R6svy+PzAvNZrsfnRUnuSfK1Np/3tvqGJHe3sX2q3ThDkjPb8mRbv35oW9POc0ZVdUo9GHy4/C3gXOAM4GvAeYs9rhHH/hhw9jG1vwC2t/Z24EOtfTnwb0CAi4C7F3nsbwI2AQ8e79iBs4BH2/Oq1l61hObzHuCPp+l7XnufnQlsaO+/05bKexE4B9jU2i8DvtnGvCyPzwvMZ7kenwAvbe3Tgbvbf/dbgata/W+B32/tPwD+trWvAj71QvN8oX2fimcAp9rPTWwBdrb2TuDKofrHauDLwMok5yzGAAGq6kvA0WPKcx37JcDuqjpaVU8Du4FLT/7of9YM85nJFuCWqvphVX0bmGTwPlwS78WqOlRVX2nt/wb2MfhW/rI8Pi8wn5ks9eNTVfW9tnh6exTwFuC2Vj/2+Ewdt9uAi5OEmec5o1MxAKb7uYkXenMsJQV8Psl9GXwjGmB1VR1q7SeB1a29HOY517Evhzm9vV0W2TF1yYRlNJ92ueB1DP6VueyPzzHzgWV6fJKcluR+4DCDYP0W8ExVPTfN2J4fd1v/LPAKjmM+p2IALGdvrKpNwGXAtUneNLyyBud5y/K2reU89iE3Ar8InA8cAv5ycYczN0leCnwaeGdV/dfwuuV4fKaZz7I9PlX1o6o6n8GvI1wAvHoh9nsqBsCsPzexVFXVwfZ8GPgsgzfCU1OXdtrz4dZ9OcxzrmNf0nOqqqfaH9QfA3/PT06vl/x8kpzO4C/LT1TVZ1p52R6f6eaznI/PlKp6BrgLeD2DS29T39UaHtvz427rXw58l+OYz6kYAMvy5yaSvCTJy6bawGbgQQZjn7rbYgK4vbV3AVe3OzYuAp4dOp1fKuY69juBzUlWtdP3za22JBzzGctvMDg+MJjPVe3ujA3ARuAelsh7sV0fvhnYV1UfHlq1LI/PTPNZxsdnLMnK1n4xg/9fyj4GQfDW1u3Y4zN13N4KfKGdwc00z5kt9CfeC/FgcBfDNxlcR3v3Yo9nxDGfy+AT/K8BD02Nm8G1vT3AfuA/gLPqJ3cO/HWb49eB8UUe/ycZnHb/H4Nrj1uPZ+zA7zL48GoSuGaJzefjbbwPtD9s5wz1f3ebzyPAZUvpvQi8kcHlnQeA+9vj8uV6fF5gPsv1+Pwy8NU27geBP2/1cxn8BT4J/AtwZqu/qC1PtvXnzjbPmR5+E1iSOnUqXgKSJI3AAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVP/D5VAeL+N6sg+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.hist(wordCount)\n",
    "print('Distribution of Synopsis Word Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2dc07bbf7fbdf69ee10a105a2c972b4ac0ecf5f"
   },
   "source": [
    "### Here's what a plot synopsis might look like.  Even to a human (at least one unfamiliar with the movie), it might not be obvious whether a synopsis describes a comedy or a drama.  An interesting side project would be to assess what human error rates are for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "cd946442f4f5e656aa59418455e959aa27bee54a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Broadway\n",
      "comedy\n",
      "The plot involves a newspaper reporter (Ed Sullivan, aka \"Mr. Broadway\") gathering material for his column. The plot was patterned on a similar film by columnist Walter Winchell, Broadway Through a Keyhole (1933). The Sullivan film primarily serves as a vehicle for him to escort viewers to various trendy New York nightclubs to watch celebrities.[1]\n"
     ]
    }
   ],
   "source": [
    "synNumber=random.randint(1,1000)\n",
    "print(df['Title'].loc[synNumber])\n",
    "print(df['Genre'].loc[synNumber])\n",
    "print(df['Plot'].loc[synNumber])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9b65c017ce3680e536bf2a06d370475c9c92e35"
   },
   "source": [
    "1. # EDA for News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "1fbb959d6999fc013529c9236bb91aae83fc1d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News Comedies:  5175\n",
      "News Non-Comedies:  5000\n",
      "Mean number of words in synopsis:  18\n",
      "Standard deviation number of words in synopsis:  14\n",
      "\n",
      "Mean number of words in headline:  8\n",
      "Standard deviation number of words in headline:  3\n"
     ]
    }
   ],
   "source": [
    "##Take roughly balanced sample of news dataset between comedy and non-comedy stories\n",
    "newsDF['Comedy']=(newsDF['category']=='COMEDY')\n",
    "news=pd.concat([newsDF[newsDF['Comedy']==False].sample(5000),newsDF[newsDF['Comedy']]],axis=0)\n",
    "print('News Comedies: ',news['Comedy'].value_counts().values[0])\n",
    "print('News Non-Comedies: ',news['Comedy'].value_counts().values[1])\n",
    "\n",
    "wordCountNews=newsDF['short_description'].apply(lambda x: x.count(' '))\n",
    "wordCountHeadline=newsDF['headline'].apply(lambda x: x.count(' '))\n",
    "\n",
    "print(\"Mean number of words in synopsis: \",int(wordCountNews.mean()))\n",
    "print(\"Standard deviation number of words in synopsis: \", int(wordCountNews.std()))\n",
    "print()\n",
    "print(\"Mean number of words in headline: \",int(wordCountHeadline.mean()))\n",
    "print(\"Standard deviation number of words in headline: \", int(wordCountHeadline.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0060cd9a5956b396bff4b7eb98944a9df147354c"
   },
   "source": [
    "# 3. Text processing and vectorization\n",
    "News descriptions are quite short, so concatenate with headlines to add information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f82044fa6f81ca4d2106cf860a336cf49d9b894d"
   },
   "outputs": [],
   "source": [
    "news=news.reset_index()\n",
    "news['Text']=news['short_description'].str.cat(news['headline'])\n",
    "news['OriginalText']=news['Text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d35ad55e0c6e6c26d2b17ddc42e2e44522015a9"
   },
   "source": [
    "Switching back to the movie dataset, stem each plot synopsis.  I don't stem the news articles because I don't use the news articles for the bag-of-words model, nor do I used stemmed versions for the neural net models.  Using stemmed words for those models did not improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9b732efb720c69ccfba9fab353de46a53eeb4730"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "df['StemmedPlot']=df['Plot'].str.split().apply(lambda x: ' '.join([stemmer.stem(y) for y in x]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a17f7054fee78031f56898f9dbd402d43f07f92f"
   },
   "source": [
    "Make the both datasets lower-case and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "390cb85980f7afe0694576e8e752fd62b3dcdcfb"
   },
   "outputs": [],
   "source": [
    "lower=news['Text'].str.lower()\n",
    "noStops=removeStopWords(lower)\n",
    "news['Text']=noStops\n",
    "\n",
    "#Store Original Plot for later\n",
    "df['OriginalPlot']=df['Plot']\n",
    "\n",
    "lower=df['Plot'].str.lower()\n",
    "cleaned=removeStopWords(lower)\n",
    "df['Plot']=cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cbe8a574b6897c547dce05384ee7376aa9566eba"
   },
   "source": [
    "Tokenize movie data and pad sequences to maximum length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "e0c1be56ac22ef7aadfb003f03fb9eae8631fae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length:  1536\n",
      "Found 81642 unique tokens.\n",
      "Shape of data tensor: (10343, 1536)\n"
     ]
    }
   ],
   "source": [
    "##Using non-stemmed\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(df['Plot']))\n",
    "sequences = tokenizer.texts_to_sequences(list(df['Plot']))\n",
    "maxLen=np.max([len(sequence) for sequence in sequences])\n",
    "print(\"Maximum Length: \",maxLen)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxLen)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "#print('Shape of label tensor:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "429da339ec7317403886e8384a2310614998192c"
   },
   "source": [
    "Tokenize movie data and pad sequences to maximum length **from movie data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b3a68dead43cb6942c72338cd2b66e7de35dc8a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27846 unique tokens.\n",
      "Shape of data tensor: (10175, 1536)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "newstokenizer = Tokenizer()\n",
    "newstokenizer.fit_on_texts(list(news['Text']))\n",
    "newsSequences = newstokenizer.texts_to_sequences(list(news['Text']))\n",
    "newsword_index = newstokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(newsword_index))\n",
    "newsdata = pad_sequences(newsSequences, maxlen=maxLen)\n",
    "\n",
    "#labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', newsdata.shape)\n",
    "#print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d11d747b7b8052ac0a7c3f0bf1dd8b154b7de8d2"
   },
   "source": [
    "Quick sanity check to make sure that the tokenizer worked properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "aabec182e3491f96f67aa59501937d0144975823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2885, 51850, 63, 1291, 8497, 200, 1030, 3198, 7665, 14547, 2848, 26, 162, 740, 1394, 582, 16834, 701, 4471, 3198, 627, 844, 2276, 6538, 3608, 4705, 1457, 1032, 3608, 3198, 2885, 69, 903, 51851, 15978, 10, 414, 24, 903, 454, 768, 2885, 226, 2175, 16861, 414, 2885, 114, 4857, 1457, 6405, 184, 284, 15978, 8006, 8163, 903, 2506, 214, 1291, 6698, 22056, 726, 3198, 2739, 39, 634, 3187, 2741, 768, 3198, 237, 2030, 800, 2725, 2756, 86, 24, 2885, 265, 837, 15978, 1266, 335, 141, 15979, 3198, 22057, 1455, 1313, 676, 634, 903, 86, 2258, 2885, 3198, 51852, 36, 218, 100, 3008, 405, 743, 2885, 1291, 233, 181, 70, 50, 68, 274, 54, 116]\n",
      "marion corsey's husband andrew conned small fortune vivian hepburn dedicates recovering money order so hides identity insinuates social circle vivian becoming secretary studies tactics employed sexy con artist employed vivian marion meets guy tarlow vivian's love interest however guy seems interested marion taking advantage guy's interest marion turns tables con artists and using vivian's strategy cons guy funds taken andrew reverse larceny discovered vivian enlists help judge perry romantically interested vivian attempt recover ill gotten gains end however marion able prove vivian's gambling club run honestly vivian cheater thief light evidence judge guy end pursuit marion vivian slinks away although marriage damaged may over marion andrew decide stay together friends see things work out\n",
      "0\n",
      "116\n",
      "marion corsey's husband andrew conned small fortune vivian hepburn dedicates recovering money order so hides identity insinuates social circle vivian becoming secretary studies tactics employed sexy con artist employed vivian marion meets guy tarlow vivian's love interest however guy seems interested marion taking advantage guy's interest marion turns tables con artists and using vivian's strategy cons guy funds taken andrew reverse larceny discovered vivian enlists help judge perry romantically interested vivian attempt recover ill gotten gains end however marion able prove vivian's gambling club run honestly vivian cheater thief light evidence judge guy end pursuit marion vivian slinks away although marriage damaged may over marion andrew decide stay together friends see things work out\n"
     ]
    }
   ],
   "source": [
    "##Sanity Check index is the word_index dictionary with keys reversed\n",
    "sanityCheckIndex={v: k for k, v in tokenizer.word_index.items()}\n",
    "print(sequences[500])\n",
    "print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in sequences[500]]))\n",
    "print(data[500][0])\n",
    "print(data[500][-1])\n",
    "print(' '.join([sanityCheckIndex[wordIndex] for wordIndex in data[500] if wordIndex!=0 ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "084034c6f70d1d3f2ce8caac29db4cbb472d4483"
   },
   "source": [
    "The above tokenizers will be used for the neural network models.  For the bag-of-words model, I use a TF-IDF DTM using the stemmed synopses created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0dbfe5e8dfca3cf16ef465cd1cac821f45ede5cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TF-IDF matrix:  (66867, 10343)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfs = tfidf.fit_transform(df['StemmedPlot'])\n",
    "print('Shape of TF-IDF matrix: ', tfs.T.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7744287171000a0fc766343229e91d36bf99c206"
   },
   "source": [
    "Split the data into train and test sets.  Additionally, create separate training sets for training data enhanced with news data and training data consisting only of movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "676f00e61be7f878a183bdbe12258caf3dbc44e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train without news shape:  (8274, 1536)\n",
      "Y train without news shape:  (8274, 2)\n",
      "X train with news shape:  (18449, 1536)\n",
      "Y train with news shape:  (18449, 2)\n",
      "X test shape:  (2069, 1536)\n",
      "Y test shape:  (2069, 2)\n"
     ]
    }
   ],
   "source": [
    "seed=random.randint(1,1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df['GenreID'], test_size=0.2, random_state=seed)\n",
    "testIndices=y_test.index\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "y_train_small=y_train.copy()\n",
    "X_train_small=X_train.copy()\n",
    "\n",
    "y_train_add=to_categorical(news['Comedy'])\n",
    "X_train_add=newsdata\n",
    "X_train=np.concatenate([X_train,X_train_add],axis=0)\n",
    "y_train=np.concatenate([y_train, y_train_add],axis=0)\n",
    "print(\"X Train without news shape: \",X_train_small.shape)\n",
    "print(\"Y train without news shape: \",y_train_small.shape)\n",
    "print(\"X train with news shape: \",X_train.shape)\n",
    "print(\"Y train with news shape: \",y_train.shape)\n",
    "print(\"X test shape: \",X_test.shape)\n",
    "print(\"Y test shape: \",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "cc1a3770106b8972df9edf0333449ff36ea3ada5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW X Train Shape:  (8274, 66867)\n",
      "BoW Y Train Shape:  (8274, 2)\n",
      "BoW X Test Shape:  (2069, 66867)\n",
      "BoW Y Test Shape:  (2069, 2)\n"
     ]
    }
   ],
   "source": [
    "X_trainBag, X_testBag, y_trainBag, y_testBag = train_test_split(tfs, df['GenreID'], test_size=0.2, random_state=seed)\n",
    "testIndicesBag=y_testBag.index\n",
    "y_trainBag=to_categorical(y_trainBag)\n",
    "y_testBag=to_categorical(y_testBag)\n",
    "print(\"BoW X Train Shape: \",X_trainBag.shape)\n",
    "print(\"BoW Y Train Shape: \",y_trainBag.shape)\n",
    "print(\"BoW X Test Shape: \",X_testBag.shape)\n",
    "print(\"BoW Y Test Shape: \",y_testBag.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef2d5464d743e9d022fa480631e326e30c90dadf"
   },
   "source": [
    "# 4.  Bag of Words Model\n",
    "Use the TF-IDF matrix (split into training and test) from the pre-processing step to train a logistic regression binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "aa1988b4c24d8162137138cada3e780f018eda8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8274 samples, validate on 2069 samples\n",
      "Epoch 1/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.6854 - acc: 0.5804 - val_loss: 0.6805 - val_acc: 0.5640\n",
      "Epoch 2/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.6671 - acc: 0.5798 - val_loss: 0.6716 - val_acc: 0.5640\n",
      "Epoch 3/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.6521 - acc: 0.5798 - val_loss: 0.6640 - val_acc: 0.5640\n",
      "Epoch 4/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.6379 - acc: 0.5850 - val_loss: 0.6564 - val_acc: 0.5660\n",
      "Epoch 5/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.6236 - acc: 0.6056 - val_loss: 0.6487 - val_acc: 0.5824\n",
      "Epoch 6/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.6090 - acc: 0.6409 - val_loss: 0.6406 - val_acc: 0.5998\n",
      "Epoch 7/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.5937 - acc: 0.6933 - val_loss: 0.6322 - val_acc: 0.6269\n",
      "Epoch 8/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.5778 - acc: 0.7463 - val_loss: 0.6234 - val_acc: 0.6593\n",
      "Epoch 9/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.5612 - acc: 0.7844 - val_loss: 0.6144 - val_acc: 0.6791\n",
      "Epoch 10/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.5443 - acc: 0.8279 - val_loss: 0.6054 - val_acc: 0.6989\n",
      "Epoch 11/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.5271 - acc: 0.8557 - val_loss: 0.5964 - val_acc: 0.7134\n",
      "Epoch 12/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.5099 - acc: 0.8715 - val_loss: 0.5876 - val_acc: 0.7264\n",
      "Epoch 13/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4930 - acc: 0.8883 - val_loss: 0.5793 - val_acc: 0.7390\n",
      "Epoch 14/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4764 - acc: 0.8979 - val_loss: 0.5713 - val_acc: 0.7448\n",
      "Epoch 15/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4604 - acc: 0.9069 - val_loss: 0.5636 - val_acc: 0.7540\n",
      "Epoch 16/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4450 - acc: 0.9155 - val_loss: 0.5565 - val_acc: 0.7593\n",
      "Epoch 17/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4302 - acc: 0.9233 - val_loss: 0.5497 - val_acc: 0.7632\n",
      "Epoch 18/20\n",
      "8274/8274 [==============================] - 12s 1ms/step - loss: 0.4160 - acc: 0.9268 - val_loss: 0.5434 - val_acc: 0.7695\n",
      "Epoch 19/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.4025 - acc: 0.9346 - val_loss: 0.5376 - val_acc: 0.7728\n",
      "Epoch 20/20\n",
      "8274/8274 [==============================] - 11s 1ms/step - loss: 0.3897 - acc: 0.9367 - val_loss: 0.5321 - val_acc: 0.7753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fa971c940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import *\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "tf_input = Input(shape=(tfs.shape[1],), dtype='float32')\n",
    "x=Dense(len(genres),activation='sigmoid')(tf_input)\n",
    "bagOfWords = Model(tf_input, x)\n",
    "bagOfWords.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "bagOfWords.fit(X_trainBag, y_trainBag, validation_data=(X_testBag, y_testBag),epochs=20, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "946c1379ac4ca1cc6e441ecb70a1ce4f3cf0587a"
   },
   "source": [
    "# 5. GloVE embeddings\n",
    "Load the GloVE 6B 100d word embeddings in hopes that they will enhance the accuracy and training speed of the neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "95efbfbf8f6f0df726fa8493544ab26e1edbbc80"
   },
   "outputs": [],
   "source": [
    "embeddings={}\n",
    "index=0\n",
    "with open('../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt') as file:\n",
    "    for embeddingLine in file:\n",
    "        lineSplit=embeddingLine.split()\n",
    "        coefs = np.asarray(lineSplit[1:], dtype='float32')\n",
    "        embeddings[lineSplit[0]]=coefs\n",
    "        index+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a4d100f873ac0e1398e26060597e8d4c0d32758"
   },
   "source": [
    "Map the GloVE embeddings to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "bced1acaea9dbe0ffb4708c0ca288fede3e59219"
   },
   "outputs": [],
   "source": [
    "embeddings_matrix=np.zeros((len(word_index)+1,len(embeddings['a'])))\n",
    "for word,i in word_index.items():\n",
    "    if word in embeddings:\n",
    "        embeddings_matrix[i]=embeddings[word]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19b7751768289cbd87bc07ef1bd93325b710884c"
   },
   "source": [
    "Quick check to make sure that the embeddings matrix is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "89dd354df92463c72e8e175fcd534e769f1d2cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word #2:  him\n",
      "Index of him :  2\n",
      "Embbedding in embeddings list:  [ 0.042409 -0.52195   0.40389  -0.31683   0.015581]\n",
      "Embedding in embeddings matrix:  [ 0.042409   -0.52195001  0.40389001 -0.31683001  0.015581  ]\n"
     ]
    }
   ],
   "source": [
    "print('Word #2: ',sanityCheckIndex[2])\n",
    "print('Index of him : ',word_index['him'])\n",
    "print('Embbedding in embeddings list: ',embeddings['him'][:5])\n",
    "print('Embedding in embeddings matrix: ',embeddings_matrix[2][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "779412a6f2368fe4d7ae4d7ec0c003e26aec4056"
   },
   "source": [
    "Create Keras embedding layers to convert the texts to embeddings.  One uses Glove, the other doesn't.  I test models using both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "690b752c45cba88e5309e0e8761129d4595dad2e"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            len(embeddings['a']),\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=maxLen,\n",
    "                            trainable=False)\n",
    "embedding_layerNoGlove = Embedding(len(word_index) + 1,\n",
    "                            len(embeddings['a']),\n",
    "                            weights=[embeddings_matrix],\n",
    "                                   input_length=maxLen,\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b121fa82aa70b280bdf2d94601ed2db18f4cc3e7"
   },
   "source": [
    "Check to make sure embedding layer does what it should be doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "7769ae09bf961385a08c7b76ef46f418a2807c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Embeddings Result:  [[0.45009, 0.16062, 0.11566], [-0.53849, 0.55165, 0.55302], [0.002042, 0.23462, 0.56775], [-0.11619, 0.45447, -0.69216], [-0.18572, 0.30092, 0.36868]]\n",
      "Model Embeddings Result:  [[ 0.45009   0.16062   0.11566 ]\n",
      " [-0.53849   0.55165   0.55302 ]\n",
      " [ 0.002042  0.23462   0.56775 ]\n",
      " [-0.11619   0.45447  -0.69216 ]\n",
      " [-0.18572   0.30092   0.36868 ]]\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(maxLen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "embeddingOnlyModel = Model(sequence_input, embedded_sequences)\n",
    "\n",
    "print('Manual Embeddings Result: ',[list(embeddings[sanityCheckIndex[x]][:3]) if sanityCheckIndex[x] in embeddings else [0,0,0] for x in sequences[500] ][-5:])\n",
    "##print(sequences[500])\n",
    "##print([ sanityCheckIndex[l] for l in list(data[500]) if l>0 ])\n",
    "##print([ sanityCheckIndex[l] for l in list(sequences[500]) if l>0 ])\n",
    "print('Model Embeddings Result: ',embeddingOnlyModel.predict(np.array(data[500]).reshape(1,maxLen))[0,-5:,:3])\n",
    "##print(embeddings_matrix[2][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9fd824f2cae3c62176c2182190305b2b7847451"
   },
   "source": [
    "# 6. CNN Model\n",
    "Start with a 3 layer CNN to predict the genre.  Note that here I am training the word encodings ourselves, rather than using the GloVE encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "133bd08896be5e17582056b5bdf76647a0e9c650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18449 samples, validate on 2069 samples\n",
      "Epoch 1/2\n",
      "18449/18449 [==============================] - 327s 18ms/step - loss: 0.6907 - acc: 0.5366 - val_loss: 0.6564 - val_acc: 0.5916\n",
      "Epoch 2/2\n",
      "18449/18449 [==============================] - 326s 18ms/step - loss: 0.6505 - acc: 0.5741 - val_loss: 0.5947 - val_acc: 0.6844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fafdba1d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_input = Input(shape=(maxLen,), dtype='int32')\n",
    "embedded_sequences = embedding_layerNoGlove(sequence_input)\n",
    "x=Conv1D(128, 9, activation='relu')(embedded_sequences)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x = Flatten()(x)\n",
    "x=Dense(128, activation='relu')(x)\n",
    "x=Dense(len(genres),activation='softmax')(x)\n",
    "\n",
    "noGloveCNN = Model(sequence_input, x)\n",
    "noGloveCNN.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "noGloveCNN.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ecaf68d665a6523b2c6c86246ee1b9162174665"
   },
   "source": [
    "Same as above, except using GloVE embeddings.  Trains faster, but no more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "0f6b50829c35740ca57f864e768d7e54e16a409f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18449 samples, validate on 2069 samples\n",
      "Epoch 1/2\n",
      "18449/18449 [==============================] - 251s 14ms/step - loss: 0.6906 - acc: 0.5346 - val_loss: 0.6584 - val_acc: 0.6254\n",
      "Epoch 2/2\n",
      "18449/18449 [==============================] - 250s 14ms/step - loss: 0.6576 - acc: 0.5662 - val_loss: 0.6149 - val_acc: 0.6834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0faffb4908>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D,MaxPooling1D,Dense,Flatten, Dropout, Bidirectional\n",
    "sequence_input = Input(shape=(maxLen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x=Conv1D(128, 9, activation='relu')(embedded_sequences)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x=Dense(128, activation='relu')(x)\n",
    "x=Dense(len(genres),activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, x)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "X_train.shape\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e43f99b3b1e1ae6b212752a6dd20c042bb81254b"
   },
   "source": [
    "Same as above except using only the movie data, without the additional news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "b588d2974d1b609e1bf8c28de06e82401d731b33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8274 samples, validate on 2069 samples\n",
      "Epoch 1/2\n",
      "8274/8274 [==============================] - 120s 15ms/step - loss: 0.6892 - acc: 0.5894 - val_loss: 0.6551 - val_acc: 0.6003\n",
      "Epoch 2/2\n",
      "8274/8274 [==============================] - 119s 14ms/step - loss: 0.6199 - acc: 0.6466 - val_loss: 0.6121 - val_acc: 0.6810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fb0222f28>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sequence_input = Input(shape=(maxLen,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x=Conv1D(128, 9, activation='relu')(embedded_sequences)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x=Dense(128, activation='relu')(x)\n",
    "x=Dense(len(genres),activation='softmax')(x)\n",
    "\n",
    "modelSmall = Model(sequence_input, x)\n",
    "modelSmall.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "X_train.shape\n",
    "modelSmall.fit(X_train_small, y_train_small, validation_data=(X_test, y_test),epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec10d7c7eea4711c47c91ec1f4ff2021740a2e7c"
   },
   "source": [
    "# 7.  LSTM-CNN model\n",
    "Try adding an LSTM layer to the end of the CNN (starting with combined data and GloVE embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "29af26d9e05bd317b8c79fa00727bc5416153842"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18449 samples, validate on 2069 samples\n",
      "Epoch 1/2\n",
      "18449/18449 [==============================] - 253s 14ms/step - loss: 0.6912 - acc: 0.5277 - val_loss: 0.6713 - val_acc: 0.5640\n",
      "Epoch 2/2\n",
      "18449/18449 [==============================] - 250s 14ms/step - loss: 0.6717 - acc: 0.5588 - val_loss: 0.6141 - val_acc: 0.6767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fb04a6c88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from keras.layers import LSTM, Dropout,Activation, Bidirectional\n",
    "word_indices =Input(shape=(maxLen,), dtype='int32')\n",
    "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "embeddingsLSTM = embedding_layer(word_indices)   \n",
    "\n",
    "# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "# Be careful, the returned output should be a batch of sequences.\n",
    "x=Conv1D(128, 9, activation='relu')(embeddingsLSTM)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "\n",
    "X =  LSTM(128,return_sequences=False)(x)\n",
    "# Add dropout with a probability of 0.5\n",
    "X = Dropout(.65)(X)\n",
    "# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "X = Dense(len(genres),activation='softmax')(X)\n",
    "# Add a softmax activation\n",
    "X = Activation('softmax')(X)\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "LSTMmodel = Model(inputs = word_indices, outputs = X) \n",
    "LSTMmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "LSTMmodel.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aaea2e9ceed87a4b99d6c39afef1bdf8d2d003f7"
   },
   "source": [
    "Same as above, except with just the movie data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "93195747c53c56ca4dcbf83879d70eebe45db40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8274 samples, validate on 2069 samples\n",
      "Epoch 1/5\n",
      "8274/8274 [==============================] - 122s 15ms/step - loss: 0.6862 - acc: 0.5709 - val_loss: 0.6846 - val_acc: 0.5640\n",
      "Epoch 2/5\n",
      "8274/8274 [==============================] - 119s 14ms/step - loss: 0.6603 - acc: 0.6079 - val_loss: 0.6377 - val_acc: 0.6312\n",
      "Epoch 3/5\n",
      "8274/8274 [==============================] - 118s 14ms/step - loss: 0.6177 - acc: 0.6647 - val_loss: 0.6228 - val_acc: 0.6510\n",
      "Epoch 4/5\n",
      "8274/8274 [==============================] - 119s 14ms/step - loss: 0.6022 - acc: 0.6841 - val_loss: 0.6022 - val_acc: 0.6887\n",
      "Epoch 5/5\n",
      "8274/8274 [==============================] - 118s 14ms/step - loss: 0.5772 - acc: 0.7131 - val_loss: 0.6000 - val_acc: 0.6868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0fb08acb00>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dropout,Activation, Bidirectional\n",
    "word_indices =Input(shape=(maxLen,), dtype='int32')\n",
    "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "embeddingsLSTM = embedding_layer(word_indices)   \n",
    "\n",
    "# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "# Be careful, the returned output should be a batch of sequences.\n",
    "x=Conv1D(128, 9, activation='relu')(embeddingsLSTM)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "x=Conv1D(128, 9, activation='relu')(x)\n",
    "x = Dropout(.4)(x)\n",
    "x=MaxPooling1D(9)(x)\n",
    "\n",
    "X =  LSTM(128,return_sequences=False)(x)\n",
    "# Add dropout with a probability of 0.5\n",
    "X = Dropout(.65)(X)\n",
    "# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "X = Dense(len(genres),activation='softmax')(X)\n",
    "# Add a softmax activation\n",
    "X = Activation('softmax')(X)\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "LSTMmodelSmall = Model(inputs = word_indices, outputs = X) \n",
    "LSTMmodelSmall.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "LSTMmodelSmall.fit(X_train_small, y_train_small, validation_data=(X_test, y_test),epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2980acd9ca50b6ff00553c5836919f1c5a0fd1f7"
   },
   "source": [
    "# 8 Analysis and Conclusion\n",
    "Make test set predictions to compare and evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "37859bd81836d673c81d473452e1153a6a3e37f4"
   },
   "outputs": [],
   "source": [
    "CNNpreds=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "77687485257769bb91bb7d5da9e58b8cd0f4d05f"
   },
   "outputs": [],
   "source": [
    "LSTMpreds=LSTMmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "6b635a99da0425658eae8df7d68e344fe6fbb628"
   },
   "outputs": [],
   "source": [
    "noGloveCNNpreds=noGloveCNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "4b8ff4d513111cd1a9b7ce724f6a7eeb6c6432be"
   },
   "outputs": [],
   "source": [
    "bagPreds=bagOfWords.predict(X_testBag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e214031f8512f5ad5451f5e6e728f1d200c3e63e"
   },
   "source": [
    "In most train-test splits, ensembling the model predictions, weighted toward the BoW predictions, improves accuracy slightly.  But the accuracy improvement above the BoW model (the best single model) is negligible at best.  Final accuracy hovers around **77%**.  Not bad, considering that the task would probably not be straightforward even for humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "7bf16ccd417b2943d07bf4a9ec5d279f54522fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of final ensembled model:  0.7752537457709038\n"
     ]
    }
   ],
   "source": [
    "avgPreds=np.average([bagPreds,LSTMpreds,CNNpreds],weights=[.8,.2,.2],axis=0)\n",
    "avgPreds=bagPreds\n",
    "\n",
    "withPreds=pd.concat([pd.DataFrame(avgPreds),df.loc[testIndices,['OriginalPlot','Genre','Title']].reset_index()],axis=1)\n",
    "withPreds['Predicted Genre']=(withPreds[0]>withPreds[1]).replace(True,'drama').replace(False,'comedy')\n",
    "accuracy=(withPreds['Predicted Genre']==withPreds['Genre']).mean()\n",
    "print('Accuracy of final ensembled model: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35b2f3314acf6089bfae20a32291e60f75b2ae12"
   },
   "source": [
    "Confusion matrix shows that the model errors are fairly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "d5ba6ff7e439a65f1cae44735ed5212eeca3ba71"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted Genre</th>\n",
       "      <th>comedy</th>\n",
       "      <th>drama</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>comedy</th>\n",
       "      <td>562</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drama</th>\n",
       "      <td>125</td>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted Genre  comedy  drama\n",
       "Genre                         \n",
       "comedy              562    340\n",
       "drama               125   1042"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(withPreds,columns='Predicted Genre', index='Genre', aggfunc=len)['index']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "486fc00c4409f14d7e7f1f4b22d3d4b204a5ddc2"
   },
   "source": [
    "Take a look at the movies the ensemble model is most confident are comedies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "f6e16fd7ea8ee1cce86251c11ca8e432d31e5cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Major League 0.052438296377658844 comedy\n",
      "Former Las Vegas showgirl Rachel Phelps (Margaret Whitton) inherits the Cleveland Indians baseball team from her deceased husband, Donald. Phelps receives a lucrative offer to move the team to Miami, but she must first trigger the escape clause in the team's contract with the city of Cleveland. To do this, she must cause attendance at the games to fall below a certain level. She decides to replace existing players with aging veterans and inexperienced rookies in the hopes that a bad team will cause attendance to decline. They need to finish dead last to void the lease to move the team to Miami. Phelps hires Lou Brown, a manager for the Toledo Mud Hens, to manage the team.\r\n",
      "During spring training in Tucson, Brown and veteran catcher Jake Taylor discover the new team has a number of interpersonal issues as well as their own struggles with the game. Problems include prima donna third baseman Roger Dorn, the weakening arm of veteran pitcher Eddie Harris; Taylor's knee issues; the inability of outfielder Pedro Cerrano to hit curveballs; and Ricky \"Wild Thing\" Vaughn's dangerous lack of pitching control. Willie Mays Hayes, who invited himself to Spring Training, has excellent baserunning speed, but is a terrible hitter. The team starts the season on a losing streak and Brown is about to demote Vaughn to the minors until he accidentally discovers that Vaughn has poor vision. After being fitted with glasses, Vaughn's performance improves and the team begins to win. Taylor discovers that his ex-girlfriend Lynn is living in Cleveland and tries to reconcile with her despite her being engaged to another man. Phelps, angered by the team's improvement, tries to demoralize them by removing team amenities. She replaces their chartered team jet with a rickety propeller plane, refuses to fix their workout equipment, and even has the hot water to the locker room turned off. Despite her efforts the team continues to win and brings themselves into contention for the division championship.\r\n",
      "The team GM Charlie Donovan visits Brown and informs him of Phelps' plan to make the team lose. He tells Brown that no matter how well they do, they will be fired or released after the season. Brown holds a team meeting and lets the players know of Phelps' plan. Taylor speaks up and, given that they will not have jobs in Cleveland next season, tells the players they might as well win the title anyway. Brown comes up with a way to motivate them into winning enough games to capture the title by using a standing cardboard cutout model of Phelps from her showgirl days. The team succeeds in tying the division with the New York Yankees, leading to a one-game playoff to determine the champions. In the playoff game in Cleveland, the Yankees take an early lead but Cerrano is able to overcome his inability to hit a curveball and hits a home run to tie the game. In the top of the 9th with the bases loaded, Vaughn manages to strike out nemesis Clu Haywood, the Yankees' best hitter, and preserve the tie.\r\n",
      "With the game tied and the Indians down to their last out in the bottom of the ninth, Willie Mays Hayes manages a single to get on base. He then steals second base while Taylor is batting, leading Taylor to suggest a trick play. Taylor steps in to bat and points to the outfield to \"call his shot\" like Babe Ruth famously did. Yankees pitcher The Duke brushes Taylor off home plate by intentionally throwing at his head, but a stubborn Taylor gets back up and again points to the outfield. With the Yankees prepared for Taylor to swing away, he instead bunts at the same time Hayes is stealing third base. Taylor beats the throw to first base which allows Hayes to slide into home safely and win the game. As the team celebrates, Taylor spots Lynn in the stands, no longer wearing her engagement ring. The two rush to hug each other as the city celebrates the victory.\r\n",
      "The theatrical release's ending includes Rachel Phelps, apparently unable to move the team because of increased attendance, angry and disappointed about the team's success. An alternate ending on the \"Wild Thing Edition\" DVD shows a very different characterization of Phelps.[3] Lou tenders his resignation and tells Phelps that he can't in good conscience work for her after she sought to sabotage the team for her own personal gain. Phelps then tells him that, in fact, she loves the Indians and never intended to move them. However, when she inherited the club from her late husband, it was on the brink of bankruptcy. Unable to afford top flight players, she decided to take a chance on unproven players from the lower leagues, whom she personally scouted, and talented older players who were generally considered washed up. She tells Lou that she likewise felt that he was the right manager to bring the ragtag group together.\r\n",
      "Phelps conceived the Miami scheme and adopted a catty, vindictive persona to unify and motivate the team. As the players believed that she wanted the Indians to fail, she was able to conceal that the team could not afford basic amenities such as chartered jet travel behind a veil of taking them away to spite the players.\r\n",
      "Lou does not resign, but Phelps reasserts her authority by saying that if he shares any part of their conversation with anyone, she will fire him.\r\n",
      "This alternate ending was actually the original ending and was filmed and shown to test screening audiences before the movie's release. The producers said that although the twist ending worked as a resolution to the plot, they scrapped it after preview audiences responded negatively, preferring the Phelps character as a villain.\n",
      " \n",
      "Jitterbugs 0.05326707288622856 comedy\n",
      "Stan and Ollie are musicians travelling across the country as \"The Original Zoot Suit Band\". On route to their next gig, their car runs out of gas and they are rescued by Chester Wright, an inventor who has perfected a pill which will turn water into gas (in reality he is a small-time con man who simply switches a water canister with a canister of gas when the duo aren't looking).\r\n",
      "The trio make a plan to travel to the next town \"Midvale\" and after using Stan and Ollie's music to attract a crowd Chester takes the opportunity to sell his \"miracle pill\" to the masses and make a fortune. As Stan and Ollie play, Chester makes the acquaintance of a young choir singer named Susan. The trio's sales pitch is initially a success but their scam is soon uncovered when a customer returns after having poured the \"gasoline\" into the fuel tank of his car and ended up in a nasty accident. Chester prevents the angry mob from attacking Stan and Ollie by posing as a police officer arriving on the scene to arrest Stan and Ollie. Susan jumps on the back of the trio's trailer after realizing Chester still has her purse that she asked him to hold. As Susan and Chester get to know each other, Susan tells Chester that her mother was recently conned out of a large amount of money by a gang who want to use the money to bet on a big horse race. Chester has a hunch that he may know the gang and with the aid of Stan and Ollie agrees to help Susan get her mother's money back.\r\n",
      "The group returns to the town and posing as out-of-town businessmen, check into the same hotel where Henry Corcoran, the leader of the con men, is staying. In plain view of Corcoran, Ollie (going under the name \"Colonel Watterson Bixby\") pretends to deposit $20,000 in the hotel safe to make the gang think that Ollie is incredibly wealthy. Corcoron instructs his girlfriend Dorcas to seduce \"Colonel Watterson Bixby\" in her hotel room, but she accidentally ends up inviting Stan instead. When Ollie enters the hotel room, a drunken Stan thinking that it is Corcoron hides under the couch and becomes increasingly more intoxicated while he is hiding. Corcoron finally enters the room and Ollie pretends to be an undercover police officer sent to arrest Corcoron for swindling Susan's mother out of her money. Thinking he is about to be sent to jail, Corcoran agrees to give the stolen money back but reveals he only has half of the $10,000 and his partner, a man named Bennett, has the other half. Corcoron attempts to shoot Ollie and Chester but is pulled to the ground by Stan and thrown in the closet by Ollie and Chester.\r\n",
      "The group then begin their next plan to get the other half of the money by having Susan get a job as a singer at the club on board a boat run by Bennett. They then have Stan dress as a woman and pose as Susan's rich aunt and convince Bennett to invest in a show at his club which they say will make him a fortune and will only require a combined $10,000 investment from him and Susan's aunt, which agrees to. The two agree to place $5000 apiece in an envelope and place it in a safe.\r\n",
      "Bennett goes to a colleague of his, Tony Queen in order to loan the $5000 to put into the show, as he plans on stealing the envelope and keeping the $10,000 for himself. Unbeknownst to Bennett, the group plan to switch the envelopes and escape with Bennett's $5000. Stan and Ollie successfully switch Bennett's $5000 and give it to Chester to put in the hotel safe. However, when the gang realize they have been conned, they capture Stan, Ollie and Susan. They search the hotel for Chester but he is nowhere to be found, giving Stan, Ollie and Susan the impression that he was only using them to get the money for himself.\r\n",
      "The trio are taken to Bennett's club where Susan is forced back into work as a singer and dancer, and Stan and Ollie are put to work in the engine room. They manage to escape after one of the guards takes one of Chester's gas pills thinking it is for indigestion and floats to the ceiling. Stan and Ollie rescue Susan but the guard accidentally lands on the controls of the ship which send it speeding out to sea, nearly hitting several other ships.\r\n",
      "The police arrive with Chester to arrest the gang. Susan scolds Chester for running off with the money but Chester clarifies that he left the hotel to wire the money to her mother immediately, showing her a receipt as proof. Stan and Ollie arrive on deck to see Susan and Chester kiss before leaving the ship. The duo plan to leave the ship as well but suddenly the remaining gangsters come around the corner and Stan and Ollie leap overboard in order to escape.\n",
      " \n",
      "Carry On Regardless 0.054304543882608414 comedy\n",
      "Down at the local labour exchange, everyone is moaning about the lack of decent jobs, unaware that nearby Bert Handy and his secretary Miss Cooling are attempting to fill vacancies, at a new enterprise called Helping Hands. When word gets round, people are quick to visit the agency, notably Sam Twist, Francis Courtenay, Delia King, Gabriel Dimple, Lily Duveen, Mike Weston and Montgomery Infield-Hopping. Bert decides to hire them all and at first business is slow. The only customer is a man who speaks gobbledygook but since Francis (who can speak 16 languages) is not present nobody can understand him and he goes on his way. Within a few days business picks up and Delia has an assignment to try on a complete women's wardrobe for Mr Delling, a gentleman who is planning a surprise for his wife. However things get complicated when the man's wife arrives home unexpectedly.\r\n",
      "Meanwhile Sam Twist is sent to a baby-sitting job, only to find that there is not a baby to be sat, instead there is Mrs Panting, a woman who needs to make her husband jealous, succeeding in the process with Sam getting a black eye. The following day, Francis is assigned to take a pet for a walk but when he gets to the owner's house, he finds out it is a chimpanzee. He takes the chimp for a walk and soon discovers that people who work in the transport industry have an aversion to apes. They eventually end up at a chimps tea party, enjoying a nice afternoon tea. Next is Lily Duveen, who has been employed at a wine tasting evening, to collect invitation cards from the attendees. After she has performed this task, she samples some of the wines and makes a bit of a spectacle of herself.\r\n",
      "Later a man from Amalgamated Scrap-Iron arrives in the Helping Hands office. He is obviously busy as he requests that someone take his place in the queue, at the hospital outpatients department. Bert says he will get someone on the case but the chap insists that the top man does the job himself, so Bert ends up queuing at the hospital where he is mistaken for an eminent diagnostician. The next job that Francis undertakes, is in the field of photography as a model. Obviously very chuffed that he has been chosen, he is crestfallen when he discovers that the job is an advertisement for a bee-keeper's helmet. His next job is between a bickering couple. The husband can not understand his wife, who continually berates him in her native German. Thanks to Francis getting a bit emotionally involved, the wife starts speaking English and the couple make up.\r\n",
      "Lefty Vincent, a boxing friend of Bert's, pops into the office. He requires four helpers to act as seconds, for his fighter Dynamite Dan. When they get to the venue, Dan is terrified by his opponent, Mickey McGee, so pretends that he has sprained his finger. The fight is off until Gabriel takes on McGee instead. Sam is excited over his next job. Due to a mix-up, he thinks he is on a top secret spying mission to the Forth Bridge when all that is required of him is to make up a fourth in a game of bridge. When Sam gets back, he learns that the whole of Helping Hands have been engaged to demonstrate exhibits at the Ideal House exhibition. Needless to say all of the demonstrations end in calamity. Sam's next job is at an exclusive men's club, where no matter how hard he tries he can not keep silent, which is a strict rule of the establishment.\r\n",
      "Miss Cooling decides on a new filing system, for a more streamlined operation and job cards are put in cubby holes for each of the workers. Disaster strikes when the cleaner knocks the box down and puts the cards back all mixed up. Everyone gets someone else's assignment, with misunderstandings all round. Finally, the gobbledygook man turns up again and this time Francis is there to translate. He is their landlord and has been trying to inform Bert that he will have to vacate the premises, because he has had a better offer. Due to a show of unity by all the staff, the landlord agrees that they can stay, on the provision that they do something for him. His main interest is property development and he needs a house cleared and cleaned. Unfortunately the team end up demolishing the house but thankfully it turns out that it needed demolishing for a block of flats anyway, so all ends well.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for x in withPreds.sort_values(by=0)[['Title','OriginalPlot',0,'Genre']].head(3).iterrows():\n",
    "    print(x[1].Title,x[1][0],x[1]['Genre'])\n",
    "    print(x[1].OriginalPlot)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17ca4410e5f7313211afd1262b27e75ef7151df0"
   },
   "source": [
    "Movies the ensemble model is most confident are not comedies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "60ae524dbc83c1e4167807de2e2f49f1a09b9de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pressure 0.34172746539115906 drama\n",
      "Anthony is a British, black teenager named who goes by the nickname of \"Tony\". Tony was born and raised in Britain while the rest of his family â€” his mother, father, and older brother â€” were born in Trinidad in the Caribbean. This affects the family members' viewpoints about the society they live in. Tonyâ€™s mother says they, as blacks, must work hard, mind their business and respect white peopleâ€™s laws because the whites have the power. The film shows how the older generations are satisfied with living in a society ruled by the white English, which differs from the views of the younger generation. There is a disconnect between the way Tony feels about Britain and the way that his family feels, specifically his brother. Tonyâ€™s brother is active in the black power movement and is constantly discussing how blacks are treated as second-class citizens who are faced with racism in an unjust societal system. He stresses the idea of a collective effort on behalf of the blacks, as black encompasses their culture and consciousness and they must spread this consciousness. He emphasises how blacks must organise politically to deal with the situation themselves, since the government is not on their side. Tony tries to assimilate into the white-dominated society that surrounds him as well as fit in with his own family and their traditions. However, as Tony tries to assimilate and maintain his faith in a British society where he can progress, he is continuously faced with obstacles.\r\n",
      "Tony goes dancing with a white friend and then goes back to her apartment, and a white adult screams that if he does not leave she will call the police, and that the white girl should feel ashamed for bringing back a black boy. When Tony attends one of his brotherâ€™s meetings, he sees the mistreatment of blacks firsthand. Police enter the meeting forcefully and with no warrant or reason, arresting and beating up the blacks. Then, police tear apart Tonyâ€™s familyâ€™s home, searching for non-existent drugs. In addition to this, throughout the film, Tony cannot find a job that matches his educational qualifications. Events like these bring to light the forces of oppression and lead to Tonyâ€™s disillusionment with a just English society. Tony also struggles with his identity, as a black child born in England to West-Indian parents. He has a difficult time relating to his brother who was not born in Europe, while he also cannot relate to his white friends, who do not share his obstacles in England. Tonyâ€™s brother feels that all whites are evil. Tony comes to his own conclusions based on his experiences, declaring that many white people are in the same position that they, as blacks, are in, since only a handful of white people hold all the power. These whites just do not realise they are in the same position as the blacks.\n",
      " \n",
      "A Question of Adultery 0.2881421148777008 drama\n",
      "Racing car driver Mark Loring, the heir to the Loring fortune complements his competitive spirit with jealousy of his wife Mary. Enraged by the attention shown to her by a \"fan\" during an evening at a restaurant, the couple is greeted by longtime friends of Mark's family, who invites them to join them at their table. Mark declines, but relents by saying, \"just one drink\".\r\n",
      "After divulging to Mary that Mark's mother was a singer who walked out on the family when Mark \"was a baby\", Mrs. Duncan's asks Mary (who, too, was a singer), to sing a favorite old song. Mark tells Mary not to, and that he \"won't have it\", but she defies Mark and does. Miffed, Mark purposely does not light Mary's cigarette, upon which she retaliates by leaning over exposing her cleavage to Mr Duncan, who delightedly obliges.\r\n",
      "Seconds later, Mary runs to the beach and Mark follows. He asks her what the devil is she trying to do to him, and proceeds to make angry and fiery love to her. After reconciliation from the night before, Mark, once again becomes jealous when Mary receives a call from her \"fan\" who wants to return her dropped glove form the previous day's race. Mark pouts, that he \"doesn't care to share her, he never seems to have her to himself, and there's always something--somebody\". With that, Mark decides to whisk American Mary away home, to London.\r\n",
      "During their drive, Mary tells Mark that she is going to have baby, and Mark replies, \"ours, I hope\". Mary slaps Mark and they get into an accident. While recovering in separate rooms, Mark's father tells him that Mary lost their unborn child; the doctor informs Mary that Mark is sterile. When the couple comes together, they console each other, however, Mark is unaware of his condition. Mark's father anticipates and hopes the \"affair\" will end, and attempts to buy Mary off. But she holds firm and tells Sir John she's not for sale, that she and her husband needs to be alone, and what Mark doesn't know is that his father is the enemy.\r\n",
      "Mary learns that Mark's father later revealed that he was sterile when Mary tells Mark she wants a baby. She comes up with the idea of artificial insemination and attempts to resolve their difficulties by travelling with Mark to a clinic in Switzerland. Soon pregnant, and though Mark had initially agreed, he becomes alienated from the idea when he thinks Mary is having an affair with a local skier who helps Mary to his cabin when she injures her ankle on the ski slopes.\r\n",
      "Returning to London alone, Mark and his father Sir John Loring take Mary to court for divorce on the charge of artificial insemination being a case of adultery. Undeterred, Mary decides to fight to preserve the reputation of her unborn child, and to confirm why another child would bring the two closer together; she only wanted her husband loveâ€”although the prosecution cried material benefit to Mrs. Lording.\r\n",
      "During proceedings Mary's attorney encouraged her to continue when she wanted to quit. He needed her permission to re-examine her husband's character (jealousy). Upon cross, Mark's personality was brought into cler view; establishing that he didn't like Mary being civil, accepting a light from a friend, singing in a nightclub or offers of hospitality when there was no other alternative. It was established that Mark was jealous from the very outset of the marriage and that the divorce proceedings were motivated purely and simply by his unreasonable and uncontrollable jealousy.\r\n",
      "Mark was also reminded that he made a spectacle of his wife during the Iberian Gran Prix, when, at the Hotel Playa, he forced himself on his wife while other eyes were watching from a terrace overlooking the public beachâ€”even though he was aware, yet his wife unwilling. Mark's actions were further put on trial when it was noted that he told his wife he would \"make up\" their loss of the baby, that he signed a document agreeing to wife's treatment of artificial inseminationâ€”but later changed his mind (without saying so), and he made love to her after she first told him she was pregnant. This proved he condoned and accepted her \"condition\".\r\n",
      "Sir John was also cross examined about his feelings and objections (similar to those he felt of his former wife) toward his son's show-biz, theater, singer wife. And it was uncovered that a bribe was offered. Sir John claimed he was protecting his son by informing him of his sterility, his passing infatuation with his wife, and the protection of the Loring Estate. In deep contemplation after the senior Loring's testimony, Mark exited the coutroom for a cigarette without a word to his father.\r\n",
      "Dr. Cameron's was next to defend his position. The prosecution's claim of \"technical adultery\" by artificial insemination was struck down as not being adultery at all. Producing a baby \"artificially\" via \"test tube\" and not another via intercourse was not the same as adultery. Moreover, the definition of adultery also negated the very act of which Mary was accused.\r\n",
      "In the end, Mark stands up to his father, finally realizing he's made a mess of his marriage and recognizes his father as the controlling figure who plays God. He walks out telling his father there will be no more dinners. Back at court, Dieter offers his assistance to Mary, if ever in need. Mark and Mary meet while waiting the decision and tells her, he will always love her.\r\n",
      "The verdict could not be read, as the jurors could never agree. Mark refuses a retrial and says he was completely wrong and should never \"have brought it\". Through his attorney, Mark begs the court's indulgence, apologizes for the trouble he has caused, withdrawals the charges, and ask the judge to dismiss the petition.\r\n",
      "The response: \"I find this an imminently most satisfactory ending\".Closing shot with Mark waiting for and receiving her as they walk together with \"Strange Affair\" playing in the background.\n",
      " \n",
      "The Indian Runner 0.3242170214653015 drama\n",
      "The story, set in 1960s Nebraska, involves two very different brothers: small-town deputy sheriff Joe and criminal Frank Roberts.\r\n",
      "Before the events of the film, Joe had tried to farm for a living, but was unable to make ends meet, and the bank eventually foreclosed on his property. He became a deputy sheriff as a way to support his young wife, Maria, and child. Joe is a good, conscientious man, but has his own demons to fight with. The opening shot of the film shows a car chase which ends with Joe using his gun to kill a man in self-defense. This results in Joe's conflicted feelings about killing the criminal, as well as the praise and scorn from members of his community from this shooting. Frank, who had been involved with run-ins with the law before going to Vietnam, is described by his father as plagued by \"restlessness\". Upon his return to town, he breaks into his brother's home and is nearly shot by Joe's wife. The next day, Frank leaves town without ever stopping by his parents' home. As Joe states in the narration, Frank was correct in his assessment that his parents would understand, as they always seem to when he hurts those who love him.\r\n",
      "Joe does not hear from his brother for some time, but eventually discovers from their father that he is in jail in another state. He had kept the information quiet to avoid upsetting their mother. Frank is then released from prison and returns to his hometown with his pregnant girlfriend, Dorothy. Joe's and Frank's mother dies and their father commits suicide soon after. Frank tries to settle down and works in construction, but keeps getting into trouble with the law, which puts him in conflict with Joe. When the time comes for Frank's wife to give birth, Frank is in a bar \"drinking it down,\" which sparks a confrontation with Joe. After Joe leaves, Frank beats the bartender to death with a chair and drives out of town with Joe on his tail. The film concludes with Joe allowing Frank to escape across the state line.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for x in withPreds.sort_values(by=1)[['Title','OriginalPlot',0,'Genre']].head(3).iterrows():\n",
    "    print(x[1].Title,x[1][0],x[1]['Genre'])\n",
    "    print(x[1].OriginalPlot)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d6ffddfac5d53bb33e4ed8965c451d769f3a190"
   },
   "source": [
    "Get the weights out of the Bag of Words model to see what words are most predictive of dramas and comedies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "4a868b6727604fbda82108c8764b76cb38fbc373"
   },
   "outputs": [],
   "source": [
    "weights=bagOfWords.get_weights()[0][:,0]\n",
    "mostDramatic=weights.argsort()[-10:][::-1]\n",
    "leastDramatic=weights.argsort()[:10][::1]\n",
    "index_to_words={v: k for k, v in tfidf.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "1b940d49417e5313a5135651504ad0fcc08887a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most likely to indicate comedy:  ['think', 'chase', 'plan', 'pretend', 'head', 'disguis', 'real', 'knock', 'date', 'manag']\n"
     ]
    }
   ],
   "source": [
    "print('Words most likely to indicate comedy: ', [index_to_words[x] for x in leastDramatic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_uuid": "18e5b4317c94cd35b3014825e992da833bdf2fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most likely to indicate drama:  ['death', 'die', 'rape', 'kill', 'killed', 'injur', 'emot', 'life', 'grief', 'dies']\n"
     ]
    }
   ],
   "source": [
    "print('Words most likely to indicate drama: ',[index_to_words[x] for x in mostDramatic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "231bc44f7fa6cbc0f8342dcb493f519f43b3a292"
   },
   "source": [
    "# Final Notes\n",
    "After all that, none of the fancy machine learning techniques improved the relatively straightforward Bag of Words logistic regression.  All of the model architectures I tried were about 7% less accurate than the simple model, even when incorporating GloVE embeddings and additional training data from news articles. A reminder, perhaps, that complexity and cutting edge is not necessarily always better, especially in light of the interpretability of the Bag of Words model, highlighted by the above list of words associated with comedy and drama synopses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4e27807ca215a9682cd06a80f683b1b09a987642"
   },
   "source": [
    "# Predict Your Own - Just for Fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "9af940bcd3fa9745b94899087a14854bfacc70c3"
   },
   "outputs": [],
   "source": [
    "def bowPredict(syn):\n",
    "    noStops=removeStopWords([syn])\n",
    "    stemmed=' '.join([stemmer.stem(x) for x in noStops[0].split() ])\n",
    "    mat=tfidf.transform([stemmed])\n",
    "    preds=bagOfWords.predict(mat)\n",
    "    if preds[0][0]>preds[0][1]: return 'comedy'\n",
    "    return 'drama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "19027df7c866d6737a0ac1ea8df9743ccfef4f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comedy'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowPredict(\"United States Naval Aviator LT Pete â€˜Maverickâ€™ Mitchell and his Radar Intercept Officer LTJG Nick â€˜Gooseâ€™ Bradshaw fly the F-14A Tomcat aboard USS Enterprise (CVN-65). During an interception with two hostile MiG-28aircraft (portrayed by a Northrop F-5), Maverick gets missile lock on one, while the other hostile aircraft locks onto Maverick's wingman, Cougar. While Maverick drives off the remaining MiG-28, Cougar is too shaken to land, and Maverick, defying orders, shepherds him back to the carrier. Cougar gives up his wings, citing his newborn child that he has never seen. Despite his dislike for Maverick's recklessness, CAG â€™Stingerâ€™ sends him and Goose to attend Topgun,[6] the Naval Fighter Weapons School at Naval Air Station Miramar.At a bar the day before Topgun starts, Maverick, assisted by Goose, unsuccessfully approaches a woman. He learns the next day that she is Charlotte â€˜Charlieâ€™ Blackwood, an astrophysicist and civilian Topgun instructor. She becomes interested in Maverick upon learning of his inverted maneuver with the MiG-28, which disproves US intelligence on the enemy aircraft's performance. During Maverick's first training sortie he defeats instructor LCDR Rick â€˜Jesterâ€™ Heatherly but through reckless flying breaks two rules of engagement and is reprimanded by chief instructor CDR Mike â€˜Viperâ€™ Metcalf. Maverick also becomes a rival to top student LT Tom â€˜Icemanâ€™ Kazansky, who considers Maverick's flying â€˜dangerous.â€™ Charlie also objects to Maverick's aggressive tactics but eventually admits that she admires his flying and omitted it from her reports to hide her feelings for him, and the two begin a romantic relationship. During a training sortie, Maverick abandons his wingman â€˜Hollywoodâ€™ to chase Viper, but is defeated when Viper maneuvers Maverick into a position from which his wingman Jester can shoot down Maverick from behind, demonstrating the value of teamwork over individual prowess. Maverick and Iceman, now direct competitors for the Topgun Trophy, chase an A-4 in a later training engagement. Maverick pressures Iceman to break off his engagement so he can shoot it down, but Maverick's F-14 flies through the jet wash of Iceman's aircraft and suffers a flameout of both engines, going into an unrecoverable flat spin. Maverick and Goose eject, but Goose hits the jettisoned aircraft canopy head-first and is killed. Although the board of inquiry clears Maverick of responsibility for Goose's death, he is overcome by guilt and his flying skill diminishes. Charlie and others attempt to console him, but Maverick considers retiring. He seeks advice from Viper, who reveals that he served with Maverick's father Duke Mitchell on the USS Oriskany and was in the air battle in which Mitchell was killed. Contrary to official reports which faulted Mitchell, Viper reveals classified information that proves Mitchell died heroically, and informs Maverick that he can succeed if he can regain his self-confidence. Maverick chooses to graduate, though Iceman wins the Top Gun Trophy. During the graduation party, Viper calls in the newly graduated aviators with the orders to deploy. Iceman, Hollywood, and Maverick are ordered to immediately return to Enterprise to deal with a â€˜crisis situationâ€™, providing air support for the rescue of a stricken ship that has drifted into hostile waters. Maverick and Merlin (Cougar's former RIO) are assigned as back-up for F-14s flown by Iceman and Hollywood, despite Iceman's reservations over Maverick's state of mind. The subsequent hostile engagement with six MiGs sees Hollywood shot down; Maverick is scrambled alone due to a catapult failure and nearly retreats after encountering circumstances similar to those that caused Goose's death. Upon finally rejoining Iceman, Maverick shoots down three MiGs, and Iceman one, forcing the other two to flee. Upon their triumphant return to Enterprise, Iceman and Maverick express newfound respect for each other. Offered any assignment he chooses, Maverick decides to return to Topgun as an instructor. At a bar in Miramar, Maverick and Charlie reunite.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
